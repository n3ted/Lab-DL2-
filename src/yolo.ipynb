{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to 'C:\\Users\\watts\\fiftyone\\coco-2017\\train' if necessary\n",
      "Found annotations at 'C:\\Users\\watts\\fiftyone\\coco-2017\\raw\\instances_train2017.json'\n",
      "Sufficient images already downloaded\n",
      "Existing download of split 'train' is sufficient\n",
      "Loading existing dataset 'coco-2017-train-1000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n",
      "Downloading split 'validation' to 'C:\\Users\\watts\\fiftyone\\coco-2017\\validation' if necessary\n",
      "Found annotations at 'C:\\Users\\watts\\fiftyone\\coco-2017\\raw\\instances_val2017.json'\n",
      "Sufficient images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading existing dataset 'coco-2017-validation-500'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n",
      "WARNING:tensorflow:From c:\\Users\\watts\\Documents\\valentAIn\\Lab-DL2-\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/20\n",
      "     16/Unknown \u001b[1m43s\u001b[0m 3s/step - class_accuracy: 0.1424 - loss: 25099.6699 - objectness_accuracy: 0.8681"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\watts\\Documents\\valentAIn\\Lab-DL2-\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 4s/step - class_accuracy: 0.1461 - loss: 24586.1328 - objectness_accuracy: 0.8731 - val_class_accuracy: 0.2648 - val_loss: 9531.3398 - val_objectness_accuracy: 0.9922\n",
      "Epoch 2/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 4s/step - class_accuracy: 0.2750 - loss: 8881.7666 - objectness_accuracy: 0.9920 - val_class_accuracy: 0.2532 - val_loss: 7993.1606 - val_objectness_accuracy: 0.9922\n",
      "Epoch 3/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 4s/step - class_accuracy: 0.2821 - loss: 7855.5444 - objectness_accuracy: 0.9920 - val_class_accuracy: 0.2611 - val_loss: 7214.0400 - val_objectness_accuracy: 0.9922\n",
      "Epoch 4/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 4s/step - class_accuracy: 0.2833 - loss: 7371.8066 - objectness_accuracy: 0.9920 - val_class_accuracy: 0.2559 - val_loss: 6882.6914 - val_objectness_accuracy: 0.9922\n",
      "Epoch 5/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 4s/step - class_accuracy: 0.2818 - loss: 7178.5913 - objectness_accuracy: 0.9920 - val_class_accuracy: 0.2364 - val_loss: 6687.2783 - val_objectness_accuracy: 0.9922\n",
      "Epoch 6/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 4s/step - class_accuracy: 0.2817 - loss: 7040.6860 - objectness_accuracy: 0.9920 - val_class_accuracy: 0.2424 - val_loss: 6604.9771 - val_objectness_accuracy: 0.9922\n",
      "Epoch 7/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 4s/step - class_accuracy: 0.2810 - loss: 6945.5190 - objectness_accuracy: 0.9920 - val_class_accuracy: 0.2532 - val_loss: 6514.3696 - val_objectness_accuracy: 0.9922\n",
      "Epoch 8/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 4s/step - class_accuracy: 0.2811 - loss: 6775.9150 - objectness_accuracy: 0.9919 - val_class_accuracy: 0.2463 - val_loss: 6056.3599 - val_objectness_accuracy: 0.9912\n",
      "Epoch 9/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 4s/step - class_accuracy: 0.2963 - loss: 6401.3677 - objectness_accuracy: 0.9915 - val_class_accuracy: 0.2583 - val_loss: 6476.6577 - val_objectness_accuracy: 0.9922\n",
      "Epoch 10/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 4s/step - class_accuracy: 0.3021 - loss: 6824.6846 - objectness_accuracy: 0.9920 - val_class_accuracy: 0.2511 - val_loss: 6412.4731 - val_objectness_accuracy: 0.9922\n",
      "Epoch 11/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 4s/step - class_accuracy: 0.2979 - loss: 6697.1416 - objectness_accuracy: 0.9920 - val_class_accuracy: 0.2537 - val_loss: 6292.1274 - val_objectness_accuracy: 0.9922\n",
      "Epoch 12/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 4s/step - class_accuracy: 0.3005 - loss: 6469.2632 - objectness_accuracy: 0.9919 - val_class_accuracy: 0.2549 - val_loss: 5920.5889 - val_objectness_accuracy: 0.9922\n",
      "Epoch 13/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 4s/step - class_accuracy: 0.3131 - loss: 6043.1509 - objectness_accuracy: 0.9919 - val_class_accuracy: 0.2495 - val_loss: 5764.3311 - val_objectness_accuracy: 0.9922\n",
      "Epoch 14/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 4s/step - class_accuracy: 0.3122 - loss: 5917.2334 - objectness_accuracy: 0.9919 - val_class_accuracy: 0.2524 - val_loss: 5732.2705 - val_objectness_accuracy: 0.9922\n",
      "Epoch 15/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 4s/step - class_accuracy: 0.3203 - loss: 5850.9590 - objectness_accuracy: 0.9920 - val_class_accuracy: 0.2525 - val_loss: 5717.5278 - val_objectness_accuracy: 0.9922\n",
      "Epoch 16/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 4s/step - class_accuracy: 0.3344 - loss: 5873.2046 - objectness_accuracy: 0.9919 - val_class_accuracy: 0.2444 - val_loss: 5638.2715 - val_objectness_accuracy: 0.9922\n",
      "Epoch 17/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 4s/step - class_accuracy: 0.3374 - loss: 5773.7148 - objectness_accuracy: 0.9919 - val_class_accuracy: 0.2454 - val_loss: 5707.0869 - val_objectness_accuracy: 0.9920\n",
      "Epoch 18/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 4s/step - class_accuracy: 0.3382 - loss: 5805.3188 - objectness_accuracy: 0.9919 - val_class_accuracy: 0.2515 - val_loss: 5685.6738 - val_objectness_accuracy: 0.9922\n",
      "Epoch 19/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 4s/step - class_accuracy: 0.3448 - loss: 5886.8052 - objectness_accuracy: 0.9918 - val_class_accuracy: 0.2352 - val_loss: 5592.8247 - val_objectness_accuracy: 0.9922\n",
      "Epoch 20/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 4s/step - class_accuracy: 0.3443 - loss: 5487.4634 - objectness_accuracy: 0.9918 - val_class_accuracy: 0.2380 - val_loss: 5122.4526 - val_objectness_accuracy: 0.9922\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Input, Reshape, Lambda, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from functools import partial\n",
    "import os\n",
    "import time\n",
    "import signal\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --- Anchor Box Calculation Function ---\n",
    "def calculate_anchor_boxes(fo_dataset, num_anchors=5, image_width=416, image_height=416):\n",
    "    all_boxes = []\n",
    "    for sample in fo_dataset:\n",
    "        if sample[\"ground_truth\"] is None:\n",
    "            continue\n",
    "        for detection in sample[\"ground_truth\"].detections:\n",
    "            bbox = detection.bounding_box\n",
    "            width = bbox[2]\n",
    "            height = bbox[3]\n",
    "            all_boxes.append([width, height])\n",
    "\n",
    "    if not all_boxes:\n",
    "        raise ValueError(\"No bounding boxes found in the dataset.\")\n",
    "\n",
    "    boxes = np.array(all_boxes)\n",
    "\n",
    "    def iou_distance(box1, box2):\n",
    "        intersection_width = min(box1[0], box2[0])\n",
    "        intersection_height = min(box1[1], box2[1])\n",
    "        if intersection_width <=0 or intersection_height <=0:\n",
    "             return 1.0\n",
    "        intersection_area = intersection_width * intersection_height\n",
    "        box1_area = box1[0] * box1[1]\n",
    "        box2_area = box2[0] * box2[1]\n",
    "        union_area = box1_area + box2_area - intersection_area\n",
    "        iou = intersection_area / (union_area + 1e-16)\n",
    "        return 1.0 - iou\n",
    "\n",
    "    def kmeans_iou(boxes, k):\n",
    "        np.random.seed(42)\n",
    "        clusters = boxes[np.random.choice(boxes.shape[0], k, replace=False)]\n",
    "        last_clusters = np.zeros((boxes.shape[0],))\n",
    "        while True:\n",
    "            distances = np.array([[iou_distance(box, cluster) for cluster in clusters] for box in boxes])\n",
    "            nearest_clusters = np.argmin(distances, axis=1)\n",
    "            if (last_clusters == nearest_clusters).all():\n",
    "                break\n",
    "            for j in range(k):\n",
    "                clusters[j] = np.median(boxes[nearest_clusters == j], axis=0)\n",
    "            last_clusters = nearest_clusters\n",
    "        return clusters\n",
    "\n",
    "    anchors = kmeans_iou(boxes, num_anchors)\n",
    "    anchors = anchors[np.argsort(anchors[:, 0] * anchors[:, 1])]\n",
    "    anchors[:, 0] *= image_width\n",
    "    anchors[:, 1] *= image_height\n",
    "    return anchors\n",
    "\n",
    "# --- Constants and Dataset Loading ---\n",
    "classes_to_download = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "    \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
    "    \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\",\n",
    "    \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\",\n",
    "    \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\",\n",
    "    \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\",\n",
    "    \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\",\n",
    "    \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\",\n",
    "    \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "]\n",
    "\n",
    "NUM_CLASSES = len(classes_to_download)\n",
    "IMAGE_SIZE = (416, 416)\n",
    "BATCH_SIZE = 64\n",
    "GRID_SIZE = 13\n",
    "NUM_BOXES = 5\n",
    "INPUT_SHAPE = (416, 416, 3)\n",
    "EPOCHS = 20\n",
    "\n",
    "# Load datasets\n",
    "train_fo_dataset = fo.zoo.load_zoo_dataset(\"coco-2017\", split=\"train\", label_types=[\"detections\"], classes=classes_to_download, max_samples=1000)\n",
    "val_fo_dataset = fo.zoo.load_zoo_dataset(\"coco-2017\", split=\"validation\", label_types=[\"detections\"], classes=classes_to_download, max_samples=500)\n",
    "\n",
    "# Calculate anchors\n",
    "ANCHORS = calculate_anchor_boxes(train_fo_dataset, num_anchors=NUM_BOXES, image_width=IMAGE_SIZE[1], image_height=IMAGE_SIZE[0])\n",
    "ANCHORS = tf.constant(ANCHORS, dtype=tf.float32) / tf.constant([IMAGE_SIZE[1], IMAGE_SIZE[0]], dtype=tf.float32)  # Normalize to [0,1]\n",
    "\n",
    "# --- Signal Handling ---\n",
    "class GracefulKiller:\n",
    "    def __init__(self, model, checkpoint_filepath):\n",
    "        self.kill_now = False\n",
    "        self.model = model\n",
    "        self.checkpoint_filepath = checkpoint_filepath\n",
    "        signal.signal(signal.SIGINT, self.exit_gracefully)\n",
    "        signal.signal(signal.SIGTERM, self.exit_gracefully)\n",
    "    def exit_gracefully(self, *args):\n",
    "        print(\"\\nSaving model...\")\n",
    "        self.model.save(self.checkpoint_filepath)\n",
    "        print(\"Model saved. Exiting.\")\n",
    "        self.kill_now = True\n",
    "\n",
    "# --- Data Pipeline ---\n",
    "def get_detections(sample_id, fo_dataset):\n",
    "    sample_id_str = sample_id.numpy().decode(\"utf-8\")\n",
    "    sample = fo_dataset[sample_id_str]\n",
    "    original_width = sample.metadata.width\n",
    "    original_height = sample.metadata.height\n",
    "    new_width, new_height = IMAGE_SIZE[1], IMAGE_SIZE[0]\n",
    "    labels, boxes = [], []\n",
    "    for detection in sample[\"ground_truth\"].detections:\n",
    "        bbox = detection.bounding_box\n",
    "        x_center_orig = (bbox[0] + bbox[2]/2) * original_width\n",
    "        y_center_orig = (bbox[1] + bbox[3]/2) * original_height\n",
    "        width_orig = bbox[2] * original_width\n",
    "        height_orig = bbox[3] * original_height\n",
    "        x_center_new = x_center_orig * (new_width / original_width)\n",
    "        y_center_new = y_center_orig * (new_height / original_height)\n",
    "        width_new = width_orig * (new_width / original_width)\n",
    "        height_new = height_orig * (new_height / original_height)\n",
    "        x_center_rel = x_center_new / new_width\n",
    "        y_center_rel = y_center_new / new_height\n",
    "        width_rel = width_new / new_width\n",
    "        height_rel = height_new / new_height\n",
    "        grid_x = int(x_center_rel * GRID_SIZE)\n",
    "        grid_y = int(y_center_rel * GRID_SIZE)\n",
    "        x_center_grid = (x_center_rel * GRID_SIZE) - grid_x\n",
    "        y_center_grid = (y_center_rel * GRID_SIZE) - grid_y\n",
    "        boxes.append([grid_x, grid_y, x_center_grid, y_center_grid, width_rel, height_rel])\n",
    "        labels.append(classes_to_download.index(detection.label))\n",
    "    return tf.cast(labels, tf.int64), tf.cast(boxes, tf.float32)\n",
    "\n",
    "def load_and_preprocess_data(sample, fo_dataset, augment=True):\n",
    "    image = tf.io.read_file(sample[\"filepath\"])\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE) / 255.0\n",
    "    if augment:\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "        image = tf.image.random_contrast(image, 0.8, 1.2)\n",
    "    labels, boxes = tf.py_function(\n",
    "        func=partial(get_detections, fo_dataset=fo_dataset),\n",
    "        inp=[sample[\"id\"]],\n",
    "        Tout=(tf.int64, tf.float32)\n",
    "    )\n",
    "\n",
    "    @tf.function\n",
    "    def process_boxes(boxes, labels):\n",
    "        target = tf.zeros((GRID_SIZE, GRID_SIZE, NUM_BOXES, 5 + NUM_CLASSES))\n",
    "        for i in tf.range(tf.shape(boxes)[0]):\n",
    "            box = boxes[i]\n",
    "            label = labels[i]\n",
    "            grid_x, grid_y = tf.cast(box[0], tf.int32), tf.cast(box[1], tf.int32)\n",
    "            x_center, y_center = box[2], box[3]\n",
    "            width, height = box[4], box[5]\n",
    "\n",
    "            gt_wh = tf.stack([width, height])\n",
    "            anchor_wh = ANCHORS\n",
    "            min_wh = tf.minimum(gt_wh, anchor_wh)\n",
    "            intersection = min_wh[..., 0] * min_wh[..., 1]\n",
    "            union = width * height + anchor_wh[..., 0] * anchor_wh[..., 1] - intersection\n",
    "            ious = intersection / (union + 1e-9)\n",
    "            best_anchor = tf.argmax(ious)\n",
    "\n",
    "            tx = x_center\n",
    "            ty = y_center\n",
    "            tw = tf.math.log(width / ANCHORS[best_anchor][0] + 1e-9)\n",
    "            th = tf.math.log(height / ANCHORS[best_anchor][1] + 1e-9)\n",
    "\n",
    "            indices = [grid_y, grid_x, tf.cast(best_anchor, tf.int32)]\n",
    "            updates = tf.concat([[tx, ty, tw, th], [1.0], tf.one_hot(label, NUM_CLASSES)], axis=0)\n",
    "            target = tf.tensor_scatter_nd_update(target, [indices], [updates])\n",
    "        return target\n",
    "\n",
    "    target = tf.cond(\n",
    "        tf.shape(boxes)[0] > 0,\n",
    "        lambda: process_boxes(boxes, labels),\n",
    "        lambda: tf.zeros((GRID_SIZE, GRID_SIZE, NUM_BOXES, 5 + NUM_CLASSES))\n",
    "    )\n",
    "    return image, target\n",
    "\n",
    "def fiftyone_dataset_generator(fo_dataset):\n",
    "    for sample in fo_dataset:\n",
    "        yield {\"filepath\": sample.filepath, \"id\": str(sample.id)}\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_tf_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: fiftyone_dataset_generator(train_fo_dataset),\n",
    "    output_types={\"filepath\": tf.string, \"id\": tf.string}\n",
    ").map(\n",
    "    lambda x: load_and_preprocess_data(x, train_fo_dataset, augment=True),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_tf_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: fiftyone_dataset_generator(val_fo_dataset),\n",
    "    output_types={\"filepath\": tf.string, \"id\": tf.string}\n",
    ").map(\n",
    "    lambda x: load_and_preprocess_data(x, val_fo_dataset, augment=False),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# --- Model Construction ---\n",
    "def build_yolo_model(input_shape, num_classes, num_boxes):\n",
    "    inputs = Input(input_shape)\n",
    "    base_model = ResNet50(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "    backbone = base_model(inputs)\n",
    "    x = Conv2D(num_boxes*(5 + num_classes), 3, padding='same')(backbone)\n",
    "    x = Reshape((GRID_SIZE, GRID_SIZE, num_boxes, 5 + num_classes))(x)\n",
    "    txy = Lambda(lambda x: tf.sigmoid(x[..., 0:2]))(x)\n",
    "    twh = Lambda(lambda x: x[..., 2:4])(x)\n",
    "    obj = Lambda(lambda x: tf.sigmoid(x[..., 4:5]))(x)\n",
    "    class_probs = Lambda(lambda x: tf.sigmoid(x[..., 5:]))(x)\n",
    "    outputs = Concatenate(axis=-1)([txy, twh, obj, class_probs])\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# --- Loss Function ---\n",
    "def yolo_loss(y_true, y_pred):\n",
    "    pred_txy = y_pred[..., 0:2]\n",
    "    pred_twh = y_pred[..., 2:4]\n",
    "    pred_obj = y_pred[..., 4:5]\n",
    "    pred_class = y_pred[..., 5:]\n",
    "    \n",
    "    true_txy = y_true[..., 0:2]\n",
    "    true_twh = y_true[..., 2:4]\n",
    "    true_obj = y_true[..., 4:5]\n",
    "    true_class = y_true[..., 5:]\n",
    "    \n",
    "    obj_mask = tf.squeeze(true_obj, axis=-1)  # Fix dimension mismatch\n",
    "    \n",
    "    xy_loss = obj_mask * tf.reduce_sum(tf.square(true_txy - pred_txy), axis=-1)\n",
    "    wh_loss = obj_mask * 0.5 * tf.reduce_sum(tf.square(true_twh - pred_twh), axis=-1)\n",
    "    obj_loss = tf.keras.losses.binary_crossentropy(true_obj, pred_obj)\n",
    "    class_loss = obj_mask * tf.keras.losses.binary_crossentropy(true_class, pred_class)\n",
    "    \n",
    "    return tf.reduce_sum(xy_loss) + tf.reduce_sum(wh_loss) + tf.reduce_sum(obj_loss) + tf.reduce_sum(class_loss)\n",
    "\n",
    "# --- Custom Metrics ---\n",
    "# Add these custom metrics to your model compilation\n",
    "def objectness_accuracy(y_true, y_pred):\n",
    "    obj_mask = y_true[..., 4:5]  # Ground truth object presence (0 or 1)\n",
    "    pred_obj = y_pred[..., 4:5]  # Predicted objectness score\n",
    "    return tf.keras.metrics.binary_accuracy(obj_mask, pred_obj)\n",
    "\n",
    "def class_accuracy(y_true, y_pred):\n",
    "    obj_mask = y_true[..., 4:5]  # Only consider cells with objects\n",
    "    true_class = tf.argmax(y_true[..., 5:], axis=-1)\n",
    "    pred_class = tf.argmax(y_pred[..., 5:], axis=-1)\n",
    "    matches = tf.cast(tf.equal(true_class, pred_class), tf.float32)\n",
    "    return tf.reduce_sum(matches * tf.squeeze(obj_mask, -1)) / (tf.reduce_sum(obj_mask) + 1e-8)\n",
    "\n",
    "# --- Training Setup ---\n",
    "checkpoint_filepath = \"models/checkpoints/best_model.keras\"\n",
    "os.makedirs(os.path.dirname(checkpoint_filepath), exist_ok=True)\n",
    "model = build_yolo_model(INPUT_SHAPE, NUM_CLASSES, NUM_BOXES)\n",
    "\n",
    "# Modify model compilation to include metrics\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=yolo_loss,\n",
    "    metrics=[\n",
    "        objectness_accuracy,\n",
    "        class_accuracy\n",
    "        # Removed MeanIoU metric as it causes scatter indexing error.\n",
    "    ]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(checkpoint_filepath, save_best_only=True, monitor='val_loss'),\n",
    "    EarlyStopping(monitor='val_loss', patience=5),\n",
    "    TensorBoard(log_dir='./logs')\n",
    "]\n",
    "\n",
    "killer = GracefulKiller(model, checkpoint_filepath)\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(EPOCHS):\n",
    "    if killer.kill_now:\n",
    "        break\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    try:\n",
    "        model.fit(train_tf_dataset,\n",
    "                   validation_data=val_tf_dataset, \n",
    "                   epochs=1, \n",
    "                   callbacks=callbacks)\n",
    "    except KeyboardInterrupt:\n",
    "        killer.exit_gracefully()\n",
    "        break\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
